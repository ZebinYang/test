======================================================
LIME (Local Interpretable Model-Agnostic Explanation)
======================================================

LIME (Local Interpretable Model-Agnostic Explanation) [M2016]_ is an explanation technique that explains the predictions of any classifier in an interpretable and faithful manner by 
learning an interpretable model locally around the prediction. 

The intuition behind this approach is quite simple and intuitive. Let's assume we have a black box model where we can input data points and get the predictions of the model. We 
seek to understand why our model made a certain prediction. LIME tests what happens to the predictions when we give variations to the input data of the machine learning model. 
LIME generates a new dataset consisting of perturbed samples and the corresponding predictions of the black box model. On this new dataset LIME then trains an interpretable model, 
which is weighted by the proximity of the sampled instances to the instance of interest. Some examples of interpretable models include but are not limited to Lasso or decision 
tree, linear regression, and logistic regression. 




**Example:**

We assume the user performed the following tasks: Data loading, preparation, and exploration. To perform a local explainability of a model, we run ``exp.model_explain()``, where 
``exp=Experiment()`` instantiate an experiment( Note: Refer to intermediate steps in other section). For illustration purposes, we consider training three models.




Choose model(s), customize if needed, click Run to train, then register the trained models one by one.

.. prompt:: pythonm $

   exp.model_train()


.. image:: images_exp/select_3_models_A.png
   :width: 800
   :alt: Alternative text

.. image:: images_exp/select_3_models_B.png
   :width: 800
   :alt: Alternative text


For Model-agnostic post-hoc explanability: global methods (PFI, PDP, ALE) and local methods (LIME, SHAP) run ``exp.model_explain()``.We explored 3 models (Tree, XGB2 and
ReLU_DNN). Select a nodel fron your list to get the following outpus.

.. prompt:: pythonm $

   exp.model_explain()

**Tree:**

.. image:: images_exp/shap_tree_0.png
   :width: 800
   :alt: Alternative text

.. image:: images_exp/shap_tree_333.png
   :width: 800
   :alt: Alternative text


**XGB2:**

.. image:: images_exp/shap_xgb2_0.png
   :width: 800
   :alt: Alternative text
   
.. image:: images_exp/shap_xgb2_8.png
   :width: 800
   :alt: Alternative text
   
**ReLu_DNN:**

.. image:: images_exp/shap_relu_0.png
   :width: 800
   :alt: Alternative text

.. image:: images_exp/shap_relu_8.png
   :width: 800
   :alt: Alternative text



.. topic:: References

     .. [M2016] Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin.
                `Why should i trust you?" Explaining the predictions of any classifier.
                <https://dl.acm.org/doi/abs/10.1145/2939672.2939778>`_,
                Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining. 2016.

