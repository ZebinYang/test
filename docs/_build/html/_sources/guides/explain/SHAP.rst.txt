======================================
SHAP (SHapley Additive exPlanations)
======================================

Undoubtedly, the progress of machine learning models is remarkable. However, the high prediction power of such a model comes at 
the cost of complex models that even experts struggle to interpret. Bringing to the 
forefront the trade-off between accuracy and interpretability of a modelâ€™s output. One approach to address this problem is to consider SHAP (SHapley Additive exPlanations) [M2017]_ , 
which assigns each feature an importance value for a particular prediction. 

**Definition:**

SHAP is used to explain the prediction of an instance x by computing the contribution of each feature to the prediction. The approach consists of computing 
Shapley values from coalitional game theory. Shapley values tell us how to fairly distribute the prediction among the features.

 
**Example:**
We assume the user performed the following tasks: Data loading, preparation, and exploration. To perform a local explainability of a model, we run ``exp.model_explain()``, where ``exp = Experiment()`` is used to 
instantiate an experiment( Note: Refer to intermediate steps in other section). For illustration purposes, we consider training three models.  




Choose model(s), customize if needed, click Run to train, then register the trained models one by one.

.. prompt:: pythonm $
  
   exp.model_train()


.. image:: images_exp/select_3_models_A.png
   :width: 800
   :alt: Alternative text

.. image:: images_exp/select_3_models_B.png
   :width: 800
   :alt: Alternative text


For Model-agnostic post-hoc explanability: global methods (PFI, PDP, ALE) and local methods (LIME, SHAP) run ``exp.model_explain()``.We explored 3 models (Tree, XGB2 and 
ReLU_DNN). Select a nodel fron your list to get the following outpus.

.. prompt:: pythonm $

   exp.model_explain()

**Tree:**

.. image:: images_exp/shap_tree_0.png
   :width: 800
   :alt: Alternative text

.. image:: images_exp/shap_tree_333.png
   :width: 800
   :alt: Alternative text

**XGB2:**

.. image:: images_exp/shap_xgb2_0.png
   :width: 800
   :alt: Alternative text

.. image:: images_exp/shap_xgb2_8.png
   :width: 800
   :alt: Alternative text

**ReLu_DNN:**

.. image:: images_exp/shap_relu_0.png
   :width: 800
   :alt: Alternative text

.. image:: images_exp/shap_relu_8.png
   :width: 800
   :alt: Alternative text





   
.. topic:: References
     
     .. [M2017] Lundberg, Scott M., and Su-In Lee.
               `A unified approach to interpreting model predictions.
               <https://proceedings.neurips.cc/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html>`_,
               Advances in neural information processing systems 30 (2017).
