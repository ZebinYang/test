======================================
PFI (Permutation Feature Importance)
======================================

..

The Permutation feature importance(PFI) of a model is defined 
as the increase in loss L when the feature set (usually just one feature) is permuted [L2001]_.
In other words, it is the decrease in a model score when a single feature value is randomly shuffled, 
breaking the relationship between the feature and the target. The drop in the model performance is indicative of how much the model depends on the feature. 
With different permutations of the feature, such a technique can be used to evaluate how the model depends on each feature of the input set
However, Features of low importance for a bad model (low cross-validation score) could be very important for a good model. Therefore for good practice, it is recommended to evaluate the predictive power of a model using a held-out set (or better with cross-validation) before computing importance. Permutation importance does not reflect the intrinsic predictive value of a feature by itself but how important this feature is for a particular model (skylearn documentation).

**Examples:**

In this example, we assume the reader has full knowledge of PiMl, starting from data preparation through model training. Therefore, we start the tutorial with model training.
In this tutorial, we focus on the prediction of the Bikesharing data using XGB2. 

**Training model:**  To train a model , run ``exp.model_train()``. in this case, we select ``XGB2`` 
and the evaluation loss is the **MSE** as follows.

.. prompt:: python $
   
   exp.model_train()

.. image:: images_exp/select_model_A.png
  :width: 800
  :alt: Alternative text

.. image:: images_exp/select_model_B.png
  :width: 800
  :alt: Alternative text

**NB:** it is also possible to customize your model and the training process. So we strongly recommend you explore the options above. 

**PFI:** Run ``exp.model_explain()`` as follow. 

.. prompt:: python $

   exp.model_explain()

.. image:: images_exp/explainability1.png
  :width: 800
  :alt: Alternative text


This analysis gives us valuable insights into how our model makes predictions. We see that the variable **hr** is the most 
important feature. **yr**, **temp**, **workingday**, and **atemp** also appear to be important predictors. Therefore,  permuting the values of these features will lead to the most decrease in the accuracy score of the model on the test set.

**Outline of the permutation algorithm:**

.. image:: images_exp/pfl_algorithm.png
  :width: 800
  :alt: Alternative text

Note: the outine above is from the scikit-learn documentation [P2011]_


.. topic:: References

        
     .. [L2001] Breiman L,
               `(2001) Random forests. Machine Learning
               <https://link.springer.com/article/10.1023/a:1010933404324>`_,
               45(1):5–32


     .. [P2011] Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, 
                Peter Prettenhofer, Ron Weiss, Vincent Dubourg, and others
                `Scikit-learn: machine learning in python. the Journal of machine Learning research,
                <https://www.jmlr.org/papers/volume12/pedregosa11a/pedregosa11a.pdf?ref=https://githubhelp.com>`_,
                12:2825–2830, 2011.
